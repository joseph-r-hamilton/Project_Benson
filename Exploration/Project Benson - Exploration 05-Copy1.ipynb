{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Benson\n",
    "\n",
    "## Exploring... and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:09.654991Z",
     "start_time": "2018-01-18T18:29:08.486020Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels\n",
    "import seaborn as sns\n",
    "from numpy import linalg\n",
    "\n",
    "import math\n",
    "import patsy\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:09.817819Z",
     "start_time": "2018-01-18T18:29:09.658854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.3 :: Anaconda custom (64-bit)\r\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:09.830881Z",
     "start_time": "2018-01-18T18:29:09.822078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 0.20.3\n",
      "Numpy version: 1.13.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Pandas version:\",pd.__version__)\n",
    "print(\"Numpy version:\",np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a week and play..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:09.851908Z",
     "start_time": "2018-01-18T18:29:09.834081Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#week = '170422' # Week of 2017, April 15\n",
    "week = '170318' # Week of 2017, April 15\n",
    "datafile = 'turnstile_%s.txt' % week\n",
    "url = 'http://web.mta.info/developers/data/nyct/turnstile/%s' % datafile\n",
    "\n",
    "# Specify location to store dataframes\n",
    "mydir = '/home/joseph/ds/Projects/Project_Benson/Data'\n",
    "#mydir =\n",
    "\n",
    "df_pickle = '%s/turnstile_%s.pkl' % (mydir,week)\n",
    "hourly_pickle = '%s/turnstile_%s_hourly.pkl' % (mydir,week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input\n",
    "\n",
    "I start with the data off the MTA site.  I want a DateTime column immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:44.098507Z",
     "start_time": "2018-01-18T18:29:09.855490Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url, parse_dates = [['DATE','TIME']])\n",
    "\n",
    "# If we're just reading this in from the MTA site, we should do this correction now\n",
    "# Comment this out otherwise\n",
    "df.columns.values[-1] = 'EXITS'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if desired, read in previous work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:44.105261Z",
     "start_time": "2018-01-18T18:29:44.100456Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_pickle(df_pickle, compression='gzip')\n",
    "#hourly = pd.read_pickle(hourly_pickle, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:44.177137Z",
     "start_time": "2018-01-18T18:29:44.107567Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.set_index(['UNIT','SCP','STATION','DATE_TIME'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:44.186229Z",
     "start_time": "2018-01-18T18:29:44.179168Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resampler(x):\n",
    "    \"\"\" A function to resample time series data - to be used in groupby apply.\"\"\"\n",
    "    return (x.set_index('DATE_TIME')       # Resample based on our timestamp\n",
    "            .resample('1H')                # Set the desired time period here\n",
    "            .mean()                        # The aggregate function used to create the new sampled rows.\n",
    "            .interpolate()                 # With the new rows, use interpolate to create the data\n",
    "            .diff()                        # Now, use diff to create deltas\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data represents a pattern of polling the turnstiles on a 4-hour period.\n",
    "\n",
    "So, setting the time period for resampling to 1 hour may permmit further data analysis on an hourly basis but with the introduction of errors since we don't truly have that granularity.  Setting the time period greater than 4 hours\n",
    "drops data because the first in the series is set to N/A with the diff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:58.598889Z",
     "start_time": "2018-01-18T18:29:44.189131Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly = (df.reset_index(level=3)\n",
    " .groupby(level=[0,1,2])\n",
    " .apply(resampler)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:58.606308Z",
     "start_time": "2018-01-18T18:29:58.600982Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The diff opeation created N/A entries.  Let's set those to zero.\n",
    "hourly.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know we need to clean or drop the \"leftover\" rows from the diff operation.\n",
    "\n",
    "Things that remain:\n",
    "* Some turnstiles are actually counting backwards, believe it or not.  So we'll use *abs* to correct that.\n",
    "* When turnstiles roll over or are reset, these create enormous anomalous values after diff.  This is low frequency (way less than 1%), so let's just set those to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:59.099803Z",
     "start_time": "2018-01-18T18:29:58.608185Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There remain anomalies from a variety of causes.  Let's clean things up...\n",
    "cleanitup = lambda x: abs(x) if abs(x) < 1000 else 0\n",
    "hourly[\"ENTRIES\"] = hourly[\"ENTRIES\"].map(cleanitup)\n",
    "hourly[\"EXITS\"] = hourly[\"EXITS\"].map(cleanitup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:59.128280Z",
     "start_time": "2018-01-18T18:29:59.101679Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weekly_aggregate = hourly.groupby('STATION').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:59.134318Z",
     "start_time": "2018-01-18T18:29:59.130172Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weekly_aggregate['TOTAL'] = weekly_aggregate['ENTRIES'] + weekly_aggregate['EXITS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:29:59.158373Z",
     "start_time": "2018-01-18T18:29:59.136279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ENTRIES     EXITS      TOTAL\n",
      "STATION                                       \n",
      "34 ST-PENN STA   901454.0  775556.0  1677010.0\n",
      "GRD CNTRL-42 ST  746535.0  659928.0  1406463.0\n",
      "34 ST-HERALD SQ  662555.0  599688.0  1262243.0\n",
      "23 ST            654403.0  474423.0  1128826.0\n",
      "14 ST-UNION SQ   566554.0  494339.0  1060893.0\n",
      "TIMES SQ-42 ST   526602.0  502185.0  1028787.0\n",
      "42 ST-PORT AUTH  576523.0  401045.0   977568.0\n",
      "86 ST            493439.0  407068.0   900507.0\n",
      "FULTON ST        459475.0  376413.0   835888.0\n",
      "125 ST           446212.0  323346.0   769558.0\n"
     ]
    }
   ],
   "source": [
    "print(weekly_aggregate.sort_values('TOTAL',ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's save our work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T18:30:03.348712Z",
     "start_time": "2018-01-18T18:29:59.160526Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(df_pickle, compression='gzip')\n",
    "hourly.to_pickle(hourly_pickle, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
